{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Embedding' from 'keras.models' (/Users/anirudhkamath/opt/anaconda3/lib/python3.7/site-packages/keras/models.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-39f70b98f10d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBidirectional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPermute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRepeatVector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Embedding' from 'keras.models' (/Users/anirudhkamath/opt/anaconda3/lib/python3.7/site-packages/keras/models.py)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Dropout, Activation, Bidirectional, Concatenate, Permute, Dot, Multiply, Reshape, RepeatVector, Lambda, Flatten\n",
    "from keras import backend as K\n",
    "from keras.activations import softmax\n",
    "\n",
    "CMU_DICT_PATH = os.path.join(\n",
    "    '../input', 'cmu-pronunciation-dictionary-unmodified-07b', 'cmudict-0.7b')\n",
    "CMU_SYMBOLS_PATH = os.path.join(\n",
    "    '../input', 'cmu-pronouncing-dictionary', 'cmudict.symbols')\n",
    "\n",
    "# Skip words with numbers or symbols\n",
    "ILLEGAL_CHAR_REGEX = \"[^A-Z-'.]\"\n",
    "\n",
    "# Only 3 words are longer than 20 chars\n",
    "# Setting a limit now simplifies training our model later\n",
    "MIN_DICT_WORD_LEN = 2\n",
    "MAX_DICT_WORD_LEN = 20\n",
    "\n",
    "def load_clean_phonetic_dictionary():\n",
    "\n",
    "    def is_alternate_pho_spelling(word):\n",
    "        # No word has > 9 alternate pronounciations so this is safe\n",
    "        return word[-1] == ')' and word[-3] == '(' and word[-2].isdigit() \n",
    "\n",
    "    def should_skip(word):\n",
    "        if not word[0].isalpha():  # skip symbols\n",
    "            return True\n",
    "        if word[-1] == '.':  # skip abbreviations\n",
    "            return True\n",
    "        if re.search(ILLEGAL_CHAR_REGEX, word):\n",
    "            return True\n",
    "        if len(word) > MAX_DICT_WORD_LEN:\n",
    "            return True\n",
    "        if len(word) < MIN_DICT_WORD_LEN:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    phonetic_dict = {}\n",
    "    with open(CMU_DICT_PATH, encoding=\"ISO-8859-1\") as cmu_dict:\n",
    "        for line in cmu_dict:\n",
    "\n",
    "            # Skip commented lines\n",
    "            if line[0:3] == ';;;':\n",
    "                continue\n",
    "\n",
    "            word, phonetic = line.strip().split('  ')\n",
    "\n",
    "            # Alternate pronounciations are formatted: \"WORD(#)  F AH0 N EH1 T IH0 K\"\n",
    "            # We don't want to the \"(#)\" considered as part of the word\n",
    "            if is_alternate_pho_spelling(word):\n",
    "                word = word[:word.find('(')]\n",
    "\n",
    "            if should_skip(word):\n",
    "                continue\n",
    "\n",
    "            if word not in phonetic_dict:\n",
    "                phonetic_dict[word] = []\n",
    "            phonetic_dict[word].append(phonetic)\n",
    "\n",
    "    if IS_KAGGLE: # limit dataset to 5,000 words\n",
    "        phonetic_dict = {key:phonetic_dict[key] \n",
    "                         for key in random.sample(list(phonetic_dict.keys()), 5000)}\n",
    "    return phonetic_dict\n",
    "\n",
    "phonetic_dict = load_clean_phonetic_dictionary()\n",
    "example_count = np.sum([len(prons) for _, prons in phonetic_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "132e94f6b03eaaf77237e18ce0675db3ab2d1f87"
   },
   "outputs": [],
   "source": [
    "START_PHONE_SYM = '\\t'\n",
    "END_PHONE_SYM = '\\n'\n",
    "\n",
    "\n",
    "def char_list():\n",
    "    allowed_symbols = [\".\", \"-\", \"'\"]\n",
    "    uppercase_letters = list(string.ascii_uppercase)\n",
    "    return [''] + allowed_symbols + uppercase_letters\n",
    "\n",
    "\n",
    "def phone_list():\n",
    "    phone_list = [START_PHONE_SYM, END_PHONE_SYM]\n",
    "    with open(CMU_SYMBOLS_PATH) as file:\n",
    "        for line in file: \n",
    "            phone_list.append(line.strip())\n",
    "    return [''] + phone_list\n",
    "\n",
    "\n",
    "def id_mappings_from_list(str_list):\n",
    "    str_to_id = {s: i for i, s in enumerate(str_list)} \n",
    "    id_to_str = {i: s for i, s in enumerate(str_list)}\n",
    "    return str_to_id, id_to_str\n",
    "\n",
    "\n",
    "# Create character to ID mappings\n",
    "char_to_id, id_to_char = id_mappings_from_list(char_list())\n",
    "\n",
    "# Load phonetic symbols and create ID mappings\n",
    "phone_to_id, id_to_phone = id_mappings_from_list(phone_list())\n",
    "\n",
    "# Example:\n",
    "print('Char to id mapping: \\n', char_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_TOKEN_COUNT = len(char_to_id)\n",
    "PHONE_TOKEN_COUNT = len(phone_to_id)\n",
    "MAX_CHAR_SEQ_LEN = max([len(word) for word, _ in phonetic_dict.items()])\n",
    "MAX_PHONE_SEQ_LEN = max([max([len(pron.split()) for pron in pronuns]) for _, pronuns in phonetic_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "77b6604223e43dd722e6edf2e7948a7d3d152f5d"
   },
   "outputs": [],
   "source": [
    "def embed_word(*words):\n",
    "    tor = []\n",
    "    for word in words:\n",
    "        word = word.upper()\n",
    "        word_matrix = np.zeros((MAX_CHAR_SEQ_LEN))\n",
    "        for t, char in enumerate(word):\n",
    "            word_matrix[t] = char_to_id[char]\n",
    "        tor.append(word_matrix)\n",
    "    return np.array(tor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2073ed67e49f7cacc3b3ce0641bf24e247a97748"
   },
   "outputs": [],
   "source": [
    "def attention_model(hidden_nodes = 256, emb_size = 256):\n",
    "    # Attention Mechanism Layers\n",
    "    attn_repeat = RepeatVector(MAX_CHAR_SEQ_LEN)\n",
    "    attn_concat = Concatenate(axis=-1)\n",
    "    attn_dense1 = Dense(128, activation=\"tanh\")\n",
    "    attn_dense2 = Dense(1, activation=\"relu\")\n",
    "    attn_softmax = Lambda(lambda x: softmax(x,axis=1))\n",
    "    attn_dot = Dot(axes = 1)\n",
    "    \n",
    "    def get_context(encoder_outputs, h_prev):\n",
    "        h_prev = attn_repeat(h_prev)\n",
    "        concat = attn_concat([encoder_outputs, h_prev])\n",
    "        e = attn_dense1(concat)\n",
    "        e = attn_dense2(e)\n",
    "        attention_weights = attn_softmax(e)\n",
    "        context = attn_dot([attention_weights, encoder_outputs])\n",
    "        return context\n",
    "    \n",
    "    # Shared Components - Encoder\n",
    "    char_inputs = Input(shape=(None,))\n",
    "    char_embedding_layer = Embedding(CHAR_TOKEN_COUNT, emb_size, input_length=MAX_CHAR_SEQ_LEN)\n",
    "    encoder = Bidirectional(LSTM(hidden_nodes, return_sequences=True, recurrent_dropout=0.2))\n",
    "    \n",
    "    # Shared Components - Decoder\n",
    "    decoder = LSTM(hidden_nodes, return_state=True, recurrent_dropout=0.2)\n",
    "    phone_embedding_layer = Embedding(PHONE_TOKEN_COUNT, emb_size)\n",
    "    embedding_reshaper = Reshape((1,emb_size,))\n",
    "    context_phone_concat = Concatenate(axis=-1)\n",
    "    context_phone_dense = Dense(hidden_nodes*3, activation=\"relu\")\n",
    "    output_layer = Dense(PHONE_TOKEN_COUNT, activation='softmax')\n",
    "    \n",
    "    # Training Model - Encoder\n",
    "    char_embeddings = char_embedding_layer(char_inputs)\n",
    "    char_embeddings = Activation('relu')(char_embeddings)\n",
    "    char_embeddings = Dropout(0.5)(char_embeddings)\n",
    "    encoder_outputs = encoder(char_embeddings)\n",
    "    \n",
    "    # Training Model - Attention Decoder\n",
    "    h0 = Input(shape=(hidden_nodes,))\n",
    "    c0 = Input(shape=(hidden_nodes,))\n",
    "    h = h0 # hidden state\n",
    "    c = c0 # cell state\n",
    "    \n",
    "    phone_inputs = []\n",
    "    phone_outputs = []\n",
    "    \n",
    "    for t in range(MAX_PHONE_SEQ_LEN):\n",
    "        phone_input = Input(shape=(None,))\n",
    "        phone_embeddings = phone_embedding_layer(phone_input)\n",
    "        phone_embeddings = Dropout(0.5)(phone_embeddings)\n",
    "        phone_embeddings = embedding_reshaper(phone_embeddings)\n",
    "        \n",
    "        context = get_context(encoder_outputs, h)\n",
    "        phone_and_context = context_phone_concat([context, phone_embeddings])\n",
    "        phone_and_context = context_phone_dense(phone_and_context)\n",
    "        \n",
    "        decoder_output, h, c = decoder(phone_and_context, initial_state = [h, c])\n",
    "        decoder_output = Dropout(0.5)(decoder_output)\n",
    "        phone_output = output_layer(decoder_output)\n",
    "        \n",
    "        phone_inputs.append(phone_input)\n",
    "        phone_outputs.append(phone_output)\n",
    "    \n",
    "    training_model = Model(inputs=[char_inputs, h0, c0] + phone_inputs, outputs=phone_outputs)\n",
    "    \n",
    "   # Testing Model - Encoder\n",
    "    testing_encoder_model = Model(char_inputs, encoder_outputs)\n",
    "\n",
    "    # Testing Model - Decoder\n",
    "    test_prev_phone_input = Input(shape=(None,))\n",
    "    test_phone_embeddings = phone_embedding_layer(test_prev_phone_input)\n",
    "    test_phone_embeddings = embedding_reshaper(test_phone_embeddings)\n",
    "    \n",
    "    test_h = Input(shape=(hidden_nodes,), name='test_h')\n",
    "    test_c = Input(shape=(hidden_nodes,), name='test_c')\n",
    "    \n",
    "    test_encoding_input = Input(shape=(MAX_CHAR_SEQ_LEN, hidden_nodes*2,))\n",
    "    test_context = get_context(test_encoding_input, test_h)\n",
    "    test_phone_and_context = Concatenate(axis=-1)([test_context, test_phone_embeddings])\n",
    "    test_phone_and_context = context_phone_dense(test_phone_and_context)\n",
    "        \n",
    "    test_seq, out_h, out_c = decoder(test_phone_and_context, initial_state = [test_h, test_c])\n",
    "    test_out = output_layer(test_seq)\n",
    "    \n",
    "    testing_decoder_model = Model([test_prev_phone_input, test_h, test_c, test_encoding_input], [test_out,out_h,out_c])\n",
    "    \n",
    "    return training_model, testing_encoder_model, testing_decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5328137af52032d27e8f681b39b51a5f5bdd5db4"
   },
   "outputs": [],
   "source": [
    "def predict_attention(input_char_seq, encoder, decoder):\n",
    "    encoder_outputs = encoder.predict(input_char_seq) \n",
    "\n",
    "    output_phone_seq = np.array([[phone_to_id[START_PHONE_SYM]]])\n",
    "    \n",
    "#     h = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "#     c = np.zeros((emb_char_input_train.shape[0], 256))\n",
    "    h = np.zeros((len(input_char_seq), 256))\n",
    "    c = np.zeros((len(input_char_seq), 256))\n",
    "    \n",
    "    end_found = False \n",
    "    pronunciation = '' \n",
    "    while not end_found:\n",
    "        decoder_output, h, c = decoder.predict([output_phone_seq, h, c, encoder_outputs])\n",
    "        \n",
    "        # Predict the phoneme with the highest probability\n",
    "        predicted_phone_idx = np.argmax(decoder_output[0,:])\n",
    "        predicted_phone = id_to_phone[predicted_phone_idx]\n",
    "        \n",
    "        pronunciation += predicted_phone + ' '\n",
    "        \n",
    "        if predicted_phone == END_PHONE_SYM or len(pronunciation.split()) > MAX_PHONE_SEQ_LEN: \n",
    "            end_found = True\n",
    "        \n",
    "        # Setup inputs for next time step\n",
    "        output_phone_seq = np.array([[predicted_phone_idx]])\n",
    "        \n",
    "    return pronunciation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bcc56b91956f52ac8b4f4c46687f13d136152bf7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ATTENTION_MODEL_WEIGHTS = os.path.join(\n",
    "    '../input', 'predicting-english-pronunciations-model-weights', 'attention_model_weights.hdf5')\n",
    "\n",
    "attn_training_model, attn_testing_encoder_model, attn_testing_decoder_model = attention_model()\n",
    "attn_training_model.load_weights(ATTENTION_MODEL_WEIGHTS)\n",
    "predict_attention(embed_word('swag'), attn_testing_encoder_model, attn_testing_decoder_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
